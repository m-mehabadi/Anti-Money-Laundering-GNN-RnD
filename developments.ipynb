{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Blogs Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timestamp             0\n",
            "From Bank             0\n",
            "Account               0\n",
            "To Bank               0\n",
            "Account.1             0\n",
            "Amount Received       0\n",
            "Receiving Currency    0\n",
            "Amount Paid           0\n",
            "Payment Currency      0\n",
            "Payment Format        0\n",
            "Is Laundering         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the CSV file into a pandas dataframe\n",
        "# only load the first 10000 rows\n",
        "df = pd.read_csv(\"./modularity_aware_gae/data/IBM_AML/HI-Small_Trans.csv\", nrows=100)\n",
        "\n",
        "# Check for null values\n",
        "null_values = df.isnull().sum()\n",
        "print(null_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a mapping from account IDs to integers\n",
        "account_to_int = {account: idx for idx, account in enumerate(set(df['Account']).union(set(df['Account.1'])))}\n",
        "int_to_account = {idx: account for account, idx in account_to_int.items()}\n",
        "df2 = df.copy()\n",
        "df2['Account'] = df2['Account'].map(account_to_int)\n",
        "df2['Account.1'] = df2['Account.1'].map(account_to_int)\n",
        "\n",
        "# Create a new graph with integer node labels\n",
        "G2 = nx.from_pandas_edgelist(df2, source='Account', target='Account.1', edge_attr='Amount Paid', create_using=nx.Graph())\n",
        "\n",
        "# save the graph G to a file without edge attributes (Amount Paid)\n",
        "nx.write_edgelist(G2, \"./modularity_aware_gae/data/IBM_AML/IBM.edgelist\", data=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# working out the label set from the IBM df\n",
        "# we will consider a list of False with size of the number of unique values in the union of the two columns\n",
        "# we will first consider all the accounts that have atleast one transaction with Is Laundering = 1\n",
        "# let's set the value of the list to True for these accounts each number should\n",
        "# correspond to the index of the account in the account_to_int dictionary\n",
        "# we will then save this list to a file\n",
        "\n",
        "# Create a list of False values with the size of the number of unique accounts\n",
        "labels = [False] * len(account_to_int)\n",
        "\n",
        "# Find accounts with at least one laundering transaction\n",
        "laundering_accounts = set(df2[df2['Is Laundering'] == 1]['Account'])\n",
        "laundering_accounts = laundering_accounts.union(set(df2[df2['Is Laundering'] == 1]['Account.1']))\n",
        "\n",
        "# Set the label to True for accounts with laundering transactions\n",
        "for account in laundering_accounts:\n",
        "    labels[account] = True\n",
        "\n",
        "# Save the labels to a file\n",
        "with open(\"./modularity_aware_gae/data/IBM_AML/IBM-labels.csv\", 'w') as f:\n",
        "    for label in labels:\n",
        "        f.write(f\"{str(label)}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Moher\\Personal\\PhD\\Projects\\AML GNN\\modularity_aware_gae\\modularity_aware_gae\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "[MODULARITY-AWARE GRAPH AUTOENCODERS]\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "EXPERIMENTAL SETTING \n",
            "\n",
            "- Graph dataset: None\n",
            "- Mode name: gcn_vae\n",
            "- Number of models to train: 1\n",
            "- Number of training iterations for each model: 200\n",
            "- Learning rate: 0.01\n",
            "- Dropout rate: 0.0\n",
            "- Use of node features in the input layer: False\n",
            "- Dimension of the GCN hidden layer: 32\n",
            "- Dimension of the output layer: 16\n",
            "- lambda: 0.5\n",
            "- beta: 0.75\n",
            "- gamma: 2.0\n",
            "- s: 10\n",
            "- FastGAE: no \n",
            "\n",
            "Final embedding vectors will be evaluated on:\n",
            "- Task 2, i.e., joint community detection and link prediction\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Using custom dataset from: ../data/IBM_AML/IBM.edgelist\n",
            "Using custom labels from: ../data/IBM_AML/IBM-labels.csv\n",
            "LOADING DATA\n",
            "\n",
            "Loading custom dataset\n",
            "- Number of nodes: 116\n",
            "- Number of communities: 1\n",
            "- Use of node features: False\n",
            "Done! \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "EXPERIMENTS ON MODEL 1 / 1 \n",
            "\n",
            "STEP 1/3 - PREPROCESSING STEPS \n",
            "\n",
            "Masking some edges from the training graph, for link prediction\n",
            "(validation set: 5.0 % of edges - test set: 10.0 % of edges)\n",
            "Done! \n",
            "\n",
            "Preprocessing node features\n",
            "Done! \n",
            "\n",
            "Running the Louvain algorithm for community detection\n",
            "as a preprocessing step for the encoder\n",
            "Done! Louvain has found 73 communities \n",
            "\n",
            "Setting up the model and the optimizer\n",
            "Done! \n",
            "\n",
            "Preprocessing on message passing matrices\n",
            "Done! \n",
            "\n",
            "Initializing TF session\n",
            "Done! \n",
            "\n",
            "STEP 2/3 - MODEL TRAINING \n",
            "\n",
            "Starting training\n",
            "Iteration: 0001 Loss: 0.50856 Time: 0.14053\n",
            "Iteration: 0002 Loss: 0.49357 Time: 0.00099\n",
            "Iteration: 0003 Loss: 0.47500 Time: 0.00200\n",
            "Iteration: 0004 Loss: 0.45290 Time: 0.00100\n",
            "Iteration: 0005 Loss: 0.42733 Time: 0.00200\n",
            "Iteration: 0006 Loss: 0.39902 Time: 0.00100\n",
            "Iteration: 0007 Loss: 0.36920 Time: 0.00200\n",
            "Iteration: 0008 Loss: 0.33959 Time: 0.00100\n",
            "Iteration: 0009 Loss: 0.31193 Time: 0.00201\n",
            "Iteration: 0010 Loss: 0.28698 Time: 0.00099\n",
            "Iteration: 0011 Loss: 0.26402 Time: 0.00200\n",
            "Iteration: 0012 Loss: 0.24106 Time: 0.00100\n",
            "Iteration: 0013 Loss: 0.21638 Time: 0.00151\n",
            "Iteration: 0014 Loss: 0.18938 Time: 0.00101\n",
            "Iteration: 0015 Loss: 0.16034 Time: 0.00200\n",
            "Iteration: 0016 Loss: 0.13000 Time: 0.00200\n",
            "Iteration: 0017 Loss: 0.09921 Time: 0.00100\n",
            "Iteration: 0018 Loss: 0.06873 Time: 0.00101\n",
            "Iteration: 0019 Loss: 0.03914 Time: 0.00100\n",
            "Iteration: 0020 Loss: 0.01085 Time: 0.00100\n",
            "Iteration: 0021 Loss: -0.01580 Time: 0.00100\n",
            "Iteration: 0022 Loss: -0.04048 Time: 0.00200\n",
            "Iteration: 0023 Loss: -0.06267 Time: 0.00100\n",
            "Iteration: 0024 Loss: -0.08187 Time: 0.00200\n",
            "Iteration: 0025 Loss: -0.09767 Time: 0.00100\n",
            "Iteration: 0026 Loss: -0.10982 Time: 0.00200\n",
            "Iteration: 0027 Loss: -0.11835 Time: 0.00100\n",
            "Iteration: 0028 Loss: -0.12362 Time: 0.00100\n",
            "Iteration: 0029 Loss: -0.12625 Time: 0.00200\n",
            "Iteration: 0030 Loss: -0.12712 Time: 0.00100\n",
            "Iteration: 0031 Loss: -0.12713 Time: 0.00200\n",
            "Iteration: 0032 Loss: -0.12704 Time: 0.00200\n",
            "Iteration: 0033 Loss: -0.12739 Time: 0.00200\n",
            "Iteration: 0034 Loss: -0.12845 Time: 0.00200\n",
            "Iteration: 0035 Loss: -0.13027 Time: 0.00100\n",
            "Iteration: 0036 Loss: -0.13272 Time: 0.00200\n",
            "Iteration: 0037 Loss: -0.13562 Time: 0.00100\n",
            "Iteration: 0038 Loss: -0.13877 Time: 0.00200\n",
            "Iteration: 0039 Loss: -0.14197 Time: 0.00100\n",
            "Iteration: 0040 Loss: -0.14504 Time: 0.00100\n",
            "Iteration: 0041 Loss: -0.14789 Time: 0.00100\n",
            "Iteration: 0042 Loss: -0.15042 Time: 0.00200\n",
            "Iteration: 0043 Loss: -0.15264 Time: 0.00200\n",
            "Iteration: 0044 Loss: -0.15457 Time: 0.00100\n",
            "Iteration: 0045 Loss: -0.15626 Time: 0.00200\n",
            "Iteration: 0046 Loss: -0.15777 Time: 0.00100\n",
            "Iteration: 0047 Loss: -0.15918 Time: 0.00100\n",
            "Iteration: 0048 Loss: -0.16053 Time: 0.00100\n",
            "Iteration: 0049 Loss: -0.16184 Time: 0.00100\n",
            "Iteration: 0050 Loss: -0.16314 Time: 0.00200\n",
            "Iteration: 0051 Loss: -0.16441 Time: 0.00100\n",
            "Iteration: 0052 Loss: -0.16564 Time: 0.00200\n",
            "Iteration: 0053 Loss: -0.16682 Time: 0.00200\n",
            "Iteration: 0054 Loss: -0.16794 Time: 0.00100\n",
            "Iteration: 0055 Loss: -0.16898 Time: 0.00200\n",
            "Iteration: 0056 Loss: -0.16995 Time: 0.00100\n",
            "Iteration: 0057 Loss: -0.17086 Time: 0.00200\n",
            "Iteration: 0058 Loss: -0.17170 Time: 0.00100\n",
            "Iteration: 0059 Loss: -0.17250 Time: 0.00200\n",
            "Iteration: 0060 Loss: -0.17325 Time: 0.00100\n",
            "Iteration: 0061 Loss: -0.17398 Time: 0.00100\n",
            "Iteration: 0062 Loss: -0.17469 Time: 0.00200\n",
            "Iteration: 0063 Loss: -0.17538 Time: 0.00100\n",
            "Iteration: 0064 Loss: -0.17606 Time: 0.00200\n",
            "Iteration: 0065 Loss: -0.17673 Time: 0.00201\n",
            "Iteration: 0066 Loss: -0.17738 Time: 0.00199\n",
            "Iteration: 0067 Loss: -0.17800 Time: 0.00100\n",
            "Iteration: 0068 Loss: -0.17859 Time: 0.00100\n",
            "Iteration: 0069 Loss: -0.17915 Time: 0.00101\n",
            "Iteration: 0070 Loss: -0.17967 Time: 0.00200\n",
            "Iteration: 0071 Loss: -0.18016 Time: 0.00100\n",
            "Iteration: 0072 Loss: -0.18063 Time: 0.00100\n",
            "Iteration: 0073 Loss: -0.18108 Time: 0.00200\n",
            "Iteration: 0074 Loss: -0.18152 Time: 0.00100\n",
            "Iteration: 0075 Loss: -0.18195 Time: 0.00200\n",
            "Iteration: 0076 Loss: -0.18237 Time: 0.00200\n",
            "Iteration: 0077 Loss: -0.18278 Time: 0.00100\n",
            "Iteration: 0078 Loss: -0.18319 Time: 0.00200\n",
            "Iteration: 0079 Loss: -0.18358 Time: 0.00151\n",
            "Iteration: 0080 Loss: -0.18397 Time: 0.00100\n",
            "Iteration: 0081 Loss: -0.18435 Time: 0.00200\n",
            "Iteration: 0082 Loss: -0.18473 Time: 0.00200\n",
            "Iteration: 0083 Loss: -0.18509 Time: 0.00100\n",
            "Iteration: 0084 Loss: -0.18545 Time: 0.00200\n",
            "Iteration: 0085 Loss: -0.18580 Time: 0.00100\n",
            "Iteration: 0086 Loss: -0.18614 Time: 0.00200\n",
            "Iteration: 0087 Loss: -0.18648 Time: 0.00200\n",
            "Iteration: 0088 Loss: -0.18681 Time: 0.00100\n",
            "Iteration: 0089 Loss: -0.18714 Time: 0.00200\n",
            "Iteration: 0090 Loss: -0.18746 Time: 0.00200\n",
            "Iteration: 0091 Loss: -0.18777 Time: 0.00200\n",
            "Iteration: 0092 Loss: -0.18808 Time: 0.00100\n",
            "Iteration: 0093 Loss: -0.18838 Time: 0.00200\n",
            "Iteration: 0094 Loss: -0.18868 Time: 0.00100\n",
            "Iteration: 0095 Loss: -0.18897 Time: 0.00301\n",
            "Iteration: 0096 Loss: -0.18925 Time: 0.00199\n",
            "Iteration: 0097 Loss: -0.18953 Time: 0.00200\n",
            "Iteration: 0098 Loss: -0.18980 Time: 0.00200\n",
            "Iteration: 0099 Loss: -0.19007 Time: 0.00100\n",
            "Iteration: 0100 Loss: -0.19033 Time: 0.00200\n",
            "Iteration: 0101 Loss: -0.19059 Time: 0.00100\n",
            "Iteration: 0102 Loss: -0.19084 Time: 0.00100\n",
            "Iteration: 0103 Loss: -0.19108 Time: 0.00200\n",
            "Iteration: 0104 Loss: -0.19132 Time: 0.00100\n",
            "Iteration: 0105 Loss: -0.19155 Time: 0.00200\n",
            "Iteration: 0106 Loss: -0.19178 Time: 0.00200\n",
            "Iteration: 0107 Loss: -0.19201 Time: 0.00100\n",
            "Iteration: 0108 Loss: -0.19223 Time: 0.00200\n",
            "Iteration: 0109 Loss: -0.19246 Time: 0.00100\n",
            "Iteration: 0110 Loss: -0.19268 Time: 0.00100\n",
            "Iteration: 0111 Loss: -0.19290 Time: 0.00200\n",
            "Iteration: 0112 Loss: -0.19311 Time: 0.00100\n",
            "Iteration: 0113 Loss: -0.19333 Time: 0.00200\n",
            "Iteration: 0114 Loss: -0.19354 Time: 0.00100\n",
            "Iteration: 0115 Loss: -0.19375 Time: 0.00200\n",
            "Iteration: 0116 Loss: -0.19396 Time: 0.00100\n",
            "Iteration: 0117 Loss: -0.19416 Time: 0.00200\n",
            "Iteration: 0118 Loss: -0.19436 Time: 0.00200\n",
            "Iteration: 0119 Loss: -0.19456 Time: 0.00100\n",
            "Iteration: 0120 Loss: -0.19476 Time: 0.00200\n",
            "Iteration: 0121 Loss: -0.19496 Time: 0.00100\n",
            "Iteration: 0122 Loss: -0.19516 Time: 0.00200\n",
            "Iteration: 0123 Loss: -0.19535 Time: 0.00100\n",
            "Iteration: 0124 Loss: -0.19554 Time: 0.00200\n",
            "Iteration: 0125 Loss: -0.19572 Time: 0.00200\n",
            "Iteration: 0126 Loss: -0.19591 Time: 0.00100\n",
            "Iteration: 0127 Loss: -0.19609 Time: 0.00201\n",
            "Iteration: 0128 Loss: -0.19627 Time: 0.00200\n",
            "Iteration: 0129 Loss: -0.19645 Time: 0.00200\n",
            "Iteration: 0130 Loss: -0.19662 Time: 0.00200\n",
            "Iteration: 0131 Loss: -0.19679 Time: 0.00100\n",
            "Iteration: 0132 Loss: -0.19696 Time: 0.00200\n",
            "Iteration: 0133 Loss: -0.19712 Time: 0.00100\n",
            "Iteration: 0134 Loss: -0.19728 Time: 0.00200\n",
            "Iteration: 0135 Loss: -0.19744 Time: 0.00200\n",
            "Iteration: 0136 Loss: -0.19760 Time: 0.00100\n",
            "Iteration: 0137 Loss: -0.19775 Time: 0.00200\n",
            "Iteration: 0138 Loss: -0.19790 Time: 0.00100\n",
            "Iteration: 0139 Loss: -0.19804 Time: 0.00200\n",
            "Iteration: 0140 Loss: -0.19819 Time: 0.00150\n",
            "Iteration: 0141 Loss: -0.19833 Time: 0.00101\n",
            "Iteration: 0142 Loss: -0.19847 Time: 0.00200\n",
            "Iteration: 0143 Loss: -0.19861 Time: 0.00100\n",
            "Iteration: 0144 Loss: -0.19874 Time: 0.00200\n",
            "Iteration: 0145 Loss: -0.19887 Time: 0.00100\n",
            "Iteration: 0146 Loss: -0.19900 Time: 0.00200\n",
            "Iteration: 0147 Loss: -0.19913 Time: 0.00200\n",
            "Iteration: 0148 Loss: -0.19925 Time: 0.00200\n",
            "Iteration: 0149 Loss: -0.19938 Time: 0.00100\n",
            "Iteration: 0150 Loss: -0.19950 Time: 0.00200\n",
            "Iteration: 0151 Loss: -0.19962 Time: 0.00100\n",
            "Iteration: 0152 Loss: -0.19974 Time: 0.00200\n",
            "Iteration: 0153 Loss: -0.19986 Time: 0.00100\n",
            "Iteration: 0154 Loss: -0.19998 Time: 0.00200\n",
            "Iteration: 0155 Loss: -0.20010 Time: 0.00100\n",
            "Iteration: 0156 Loss: -0.20022 Time: 0.00200\n",
            "Iteration: 0157 Loss: -0.20033 Time: 0.00100\n",
            "Iteration: 0158 Loss: -0.20045 Time: 0.00201\n",
            "Iteration: 0159 Loss: -0.20056 Time: 0.00199\n",
            "Iteration: 0160 Loss: -0.20068 Time: 0.00100\n",
            "Iteration: 0161 Loss: -0.20079 Time: 0.00100\n",
            "Iteration: 0162 Loss: -0.20091 Time: 0.00100\n",
            "Iteration: 0163 Loss: -0.20102 Time: 0.00100\n",
            "Iteration: 0164 Loss: -0.20113 Time: 0.00200\n",
            "Iteration: 0165 Loss: -0.20125 Time: 0.00100\n",
            "Iteration: 0166 Loss: -0.20136 Time: 0.00200\n",
            "Iteration: 0167 Loss: -0.20147 Time: 0.00100\n",
            "Iteration: 0168 Loss: -0.20158 Time: 0.00200\n",
            "Iteration: 0169 Loss: -0.20169 Time: 0.00100\n",
            "Iteration: 0170 Loss: -0.20180 Time: 0.00200\n",
            "Iteration: 0171 Loss: -0.20191 Time: 0.00100\n",
            "Iteration: 0172 Loss: -0.20202 Time: 0.00100\n",
            "Iteration: 0173 Loss: -0.20213 Time: 0.00200\n",
            "Iteration: 0174 Loss: -0.20224 Time: 0.00100\n",
            "Iteration: 0175 Loss: -0.20234 Time: 0.00100\n",
            "Iteration: 0176 Loss: -0.20245 Time: 0.00200\n",
            "Iteration: 0177 Loss: -0.20256 Time: 0.00100\n",
            "Iteration: 0178 Loss: -0.20267 Time: 0.00200\n",
            "Iteration: 0179 Loss: -0.20277 Time: 0.00100\n",
            "Iteration: 0180 Loss: -0.20288 Time: 0.00200\n",
            "Iteration: 0181 Loss: -0.20298 Time: 0.00100\n",
            "Iteration: 0182 Loss: -0.20308 Time: 0.00200\n",
            "Iteration: 0183 Loss: -0.20319 Time: 0.00100\n",
            "Iteration: 0184 Loss: -0.20329 Time: 0.00200\n",
            "Iteration: 0185 Loss: -0.20339 Time: 0.00100\n",
            "Iteration: 0186 Loss: -0.20349 Time: 0.00200\n",
            "Iteration: 0187 Loss: -0.20359 Time: 0.00100\n",
            "Iteration: 0188 Loss: -0.20368 Time: 0.00200\n",
            "Iteration: 0189 Loss: -0.20378 Time: 0.00100\n",
            "Iteration: 0190 Loss: -0.20387 Time: 0.00200\n",
            "Iteration: 0191 Loss: -0.20396 Time: 0.00100\n",
            "Iteration: 0192 Loss: -0.20406 Time: 0.00200\n",
            "Iteration: 0193 Loss: -0.20415 Time: 0.00200\n",
            "Iteration: 0194 Loss: -0.20424 Time: 0.00200\n",
            "Iteration: 0195 Loss: -0.20432 Time: 0.00100\n",
            "Iteration: 0196 Loss: -0.20441 Time: 0.00100\n",
            "Iteration: 0197 Loss: -0.20450 Time: 0.00201\n",
            "Iteration: 0198 Loss: -0.20458 Time: 0.00100\n",
            "Iteration: 0199 Loss: -0.20467 Time: 0.00200\n",
            "Iteration: 0200 Loss: -0.20475 Time: 0.00100\n",
            "Done! \n",
            "\n",
            "STEP 3/3 - MODEL EVALUATION \n",
            "\n",
            "Computing the final embedding vectors, for evaluation\n",
            "Done! \n",
            "\n",
            "Testing: link prediction\n",
            "Done! \n",
            "\n",
            "Testing: community detection\n",
            "Done! \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "FINAL RESULTS \n",
            "\n",
            "Recall: the selected task was \"Task 2\", i.e., joint community detection and link prediction, on None\n",
            "All scores reported below are computed over the 1 run(s) \n",
            "\n",
            "Community detection:\n",
            "\n",
            "Mean AMI score: 1.0\n",
            "Std of AMI scores: 0.0 \n",
            "\n",
            "Mean ARI score:  1.0\n",
            "Std of ARI scores:  0.0 \n",
            "\n",
            "Link prediction:\n",
            "\n",
            "Mean AUC score:  0.64\n",
            "Std of AUC scores:  0.0 \n",
            "\n",
            "Mean AP score:  0.7166666666666666\n",
            "Std of AP scores:  0.0 \n",
            " \n",
            "\n",
            "c:\\Users\\Moher\\Personal\\PhD\\Projects\\AML GNN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-05 22:33:31.395583: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
            "2025-03-05 22:33:31.395857: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2025-03-05 22:33:33.694183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n",
            "2025-03-05 22:33:33.720031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: NVIDIA GeForce RTX 4080 major: 8 minor: 9 memoryClockRate(GHz): 2.505\n",
            "pciBusID: 0000:01:00.0\n",
            "2025-03-05 22:33:33.721169: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
            "2025-03-05 22:33:33.722109: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found\n",
            "2025-03-05 22:33:33.722983: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found\n",
            "2025-03-05 22:33:33.723910: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found\n",
            "2025-03-05 22:33:33.724816: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found\n",
            "2025-03-05 22:33:33.725658: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found\n",
            "2025-03-05 22:33:33.726493: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\n",
            "2025-03-05 22:33:33.726736: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2025-03-05 22:33:33.727511: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
            "2025-03-05 22:33:33.731605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2025-03-05 22:33:33.731812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
          ]
        }
      ],
      "source": [
        "%pwd\n",
        "%cd modularity_aware_gae/modularity_aware_gae\n",
        "!python train.py --dataset_path=../data/IBM_AML/IBM.edgelist --labelset_path=../data/IBM_AML/IBM-labels.csv --features=False --task=task_2 --model=gcn_vae --iterations=200 --learning_rate=0.01 --hidden=32 --dimension=16 --beta=0.75 --lamb=0.5 --gamma=2 --s_reg=10 --fastgae=False --nb_run=1\n",
        "%cd ../.."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "_pyenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
